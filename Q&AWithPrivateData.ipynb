{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCXGYD8BHErEudSJzkVyhf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abmishra1234/LLM-Apps/blob/Development/Q%26AWithPrivateData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project - Question, Answer on Private Documents which is owned by your Company or Individuals"
      ],
      "metadata": {
        "id": "xVjqEPA0hTVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installation Required For Project"
      ],
      "metadata": {
        "id": "XAd28wdihm5o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gylJxGrHhIZ7"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirement.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports required for Project"
      ],
      "metadata": {
        "id": "VuIFYjZlij7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=True)"
      ],
      "metadata": {
        "id": "K5qDSf9ZipKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Documents"
      ],
      "metadata": {
        "id": "kdpRbNVejPOR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loading PDF, DOCX and TXT files as LangChain Documents\n",
        "def load_document(file):\n",
        "    import os\n",
        "    name, extension = os.path.splitext(file)\n",
        "\n",
        "    if extension == '.pdf':\n",
        "        from langchain.document_loaders import PyPDFLoader\n",
        "        print(f'Loading {file}')\n",
        "        loader = PyPDFLoader(file)\n",
        "    elif extension == '.docx':\n",
        "        from langchain.document_loaders import Docx2txtLoader\n",
        "        print(f'Loading {file}')\n",
        "        loader = Docx2txtLoader(file)\n",
        "    elif extension == '.txt':\n",
        "        from langchain.document_loaders import TextLoader\n",
        "        loader = TextLoader(file)\n",
        "    else:\n",
        "        print('Document format is not supported!')\n",
        "        return None\n",
        "\n",
        "    data = loader.load()\n",
        "    return data\n",
        "\n",
        "\n",
        "# wikipedia\n",
        "def load_from_wikipedia(query, lang='en', load_max_docs=2):\n",
        "    from langchain.document_loaders import WikipediaLoader\n",
        "    loader = WikipediaLoader(query=query, lang=lang, load_max_docs=load_max_docs)\n",
        "    data = loader.load()\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "vt5hMMOcjTMv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking Data"
      ],
      "metadata": {
        "id": "VQ3GZAwCJ5e6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_data(data, chunk_size=256):\n",
        "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=0)\n",
        "    chunks = text_splitter.split_documents(data)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "9x1ZEBA_J-TT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculating the Cost"
      ],
      "metadata": {
        "id": "DmV49FV2KJK6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_embedding_cost(texts):\n",
        "    import tiktoken\n",
        "    enc = tiktoken.encoding_for_model('text-embedding-3-small')\n",
        "    total_tokens = sum([len(enc.encode(page.page_content)) for page in texts])\n",
        "    print(f'Total Tokens: {total_tokens}')\n",
        "    print(f'Embedding Cost in USD: {total_tokens / 1000 * 0.00002:.6f}')"
      ],
      "metadata": {
        "id": "evOJL7RZKLy5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding andd Uploading to a vector Database (Pinecone)"
      ],
      "metadata": {
        "id": "f4XQwT8rKg03"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def insert_or_fetch_embeddings(index_name, chunks):\n",
        "    # importing the necessary libraries and initializing the Pinecone client\n",
        "    import pinecone\n",
        "    from langchain_community.vectorstores import Pinecone\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "    from pinecone import PodSpec\n",
        "\n",
        "\n",
        "    pc = pinecone.Pinecone()\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  # 512 works as well\n",
        "\n",
        "    # loading from existing index\n",
        "    if index_name in pc.list_indexes():\n",
        "        print(f'Index {index_name} already exists. Loading embeddings ... ', end='')\n",
        "        vector_store = Pinecone.from_existing_index(index_name, embeddings)\n",
        "        print('Ok')\n",
        "    else:\n",
        "        # creating the index and embedding the chunks into the index\n",
        "        print(f'Creating index {index_name} and embeddings ...', end='')\n",
        "\n",
        "        # creating a new index\n",
        "        pc.create_index(\n",
        "            name=index_name,\n",
        "            dimension=1536,\n",
        "            metric='cosine',\n",
        "            spec=PodSpec(\n",
        "                environment='gcp-starter'\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # processing the input documents, generating embeddings using the provided `OpenAIEmbeddings` instance,\n",
        "        # inserting the embeddings into the index and returning a new Pinecone vector store object.\n",
        "        vector_store = Pinecone.from_documents(chunks, embeddings, index_name=index_name)\n",
        "        print('Ok')\n",
        "\n",
        "    return vector_store\n",
        ""
      ],
      "metadata": {
        "id": "UOKczCD1Km3Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_pinecone_index(index_name='all'):\n",
        "    import pinecone\n",
        "    pc = pinecone.Pinecone()\n",
        "\n",
        "    if index_name == 'all':\n",
        "        indexes = pc.list_indexes().names()\n",
        "        print('Deleting all indexes ... ')\n",
        "        for index in indexes:\n",
        "            pc.delete_index(index)\n",
        "        print('Ok')\n",
        "    else:\n",
        "        print(f'Deleting index {index_name} ...', end='')\n",
        "        pc.delete_index(index_name)\n",
        "        print('Ok')\n",
        ""
      ],
      "metadata": {
        "id": "9_rx4mA3K1LD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Asking and Getting Answers"
      ],
      "metadata": {
        "id": "8vx72oOHK55o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_and_get_answer(vector_store, q, k=3):\n",
        "    from langchain.chains import RetrievalQA\n",
        "    from langchain_openai import ChatOpenAI\n",
        "\n",
        "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
        "\n",
        "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
        "\n",
        "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
        "\n",
        "    answer = chain.invoke(q)\n",
        "    return answer\n",
        ""
      ],
      "metadata": {
        "id": "jIIHp16fK-Pg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install -q chromadb"
      ],
      "metadata": {
        "id": "WvT8jKAtLq_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embeddings_chroma(chunks, persist_directory='./chroma_db'):\n",
        "    from langchain.vectorstores import Chroma\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "    # Instantiate an embedding model from OpenAI (smaller version for efficiency)\n",
        "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
        "\n",
        "    # Create a Chroma vector store using the provided text chunks and embedding model,\n",
        "    # configuring it to save data to the specified directory\n",
        "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory)\n",
        "\n",
        "    return vector_store  # Return the created vector store\n"
      ],
      "metadata": {
        "id": "aYfOPfcCL6f5"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
        "    from langchain.vectorstores import Chroma\n",
        "    from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "    # Instantiate the same embedding model used during creation\n",
        "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)\n",
        "\n",
        "    # Load a Chroma vector store from the specified directory, using the provided embedding function\n",
        "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "\n",
        "    return vector_store  # Return the loaded vector store\n"
      ],
      "metadata": {
        "id": "Ag781D9wL_gp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Code"
      ],
      "metadata": {
        "id": "xxT7spYkLG44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install langchain_openai"
      ],
      "metadata": {
        "id": "tz6sA6utMRk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the pdf document into LangChain\n",
        "data = load_document('files/keph102.pdf')\n",
        "\n",
        "# Splitting the document into chunks\n",
        "chunks = chunk_data(data, chunk_size=256)\n",
        "\n",
        "# Creating a Chroma vector store using the provided text chunks and embedding model (default is text-embedding-3-small)\n",
        "vector_store = create_embeddings_chroma(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-0ruaX5LJZn",
        "outputId": "7a294575-55a8-4482-c2c7-e166547f5c25"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading files/keph102.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Asking questions\n",
        "q = 'What is the size of nucleus?'\n",
        "answer = ask_and_get_answer(vector_store, q)\n",
        "print(answer['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOyz7sykNBLC",
        "outputId": "d5ce7035-ca20-45e7-afe2-a0a4481c6479"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The size of a nucleus is in the range of 10^(-15) m to 10^(-14) m.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a Chroma vector store from the specified directory (default ./chroma_db)\n",
        "db = load_embeddings_chroma()\n",
        "q = 'What is atomic clock?'\n",
        "answer = ask_and_get_answer(vector_store, q)\n",
        "print(answer['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoVqVq8eQJoa",
        "outputId": "e88272c0-e1fa-4ed5-cde1-cb9ccf7cbfb6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An atomic clock is a highly accurate timekeeping device that uses the vibrations of atoms, typically cesium atoms, to regulate and measure time. They are considered the most accurate timepieces available and are used as standards for timekeeping around the world.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can't ask follow-up questions. There is no memory (chat history) available.\n",
        "q = 'What is the number?'\n",
        "answer = ask_and_get_answer(vector_store, q)\n",
        "print(answer['result'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-rodu4FROhb",
        "outputId": "8f97ecee-7ab1-450c-ea24-48b1b4929442"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't have enough context to determine the specific number being asked about.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding Memory ( Chat History )"
      ],
      "metadata": {
        "id": "0y3OlzAqRtXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains import ConversationalRetrievalChain  # Import class for building conversational AI chains\n",
        "from langchain.memory import ConversationBufferMemory  # Import memory for storing conversation history\n",
        "\n",
        "# Instantiate a ChatGPT LLM (temperature controls randomness)\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
        "\n",
        "# Configure vector store to act as a retriever (finding similar items, returning top 5)\n",
        "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
        "\n",
        "\n",
        "# Create a memory buffer to track the conversation\n",
        "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
        "\n",
        "crc = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,  # Link the ChatGPT LLM\n",
        "    retriever=retriever,  # Link the vector store based retriever\n",
        "    memory=memory,  # Link the conversation memory\n",
        "    chain_type='stuff',  # Specify the chain type\n",
        "    verbose=False  # Set to True to enable verbose logging for debugging\n",
        ")"
      ],
      "metadata": {
        "id": "E8b6B0BfRzHk"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a function to ask questions\n",
        "def ask_question(q, chain):\n",
        "    result = chain.invoke({'question': q})\n",
        "    return result"
      ],
      "metadata": {
        "id": "nZw-nmarR8xh"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_document('files/Chem_11_Chapter02.pdf')\n",
        "chunks = chunk_data(data, chunk_size=256)\n",
        "vector_store = create_embeddings_chroma(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqajqFGeSrPP",
        "outputId": "b8768735-a05b-4400-b0b7-4d6d60f93dbd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading files/Chem_11_Chapter02.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q = 'What is atomic clock?'\n",
        "result = ask_question(q, crc)\n",
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8LUMMmRTWAP",
        "outputId": "dda69796-ea8b-4761-954e-56c947909b78"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An atomic clock is a highly accurate timekeeping device that uses the vibrations of atoms to regulate the time. Specifically, cesium atomic clocks are commonly used as they are very accurate and provide a standard for measuring time intervals. These clocks are used in national standards laboratories to maintain precise time measurements.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item in result['chat_history']:\n",
        "    print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsMDj_EOgYfu",
        "outputId": "bfefca2a-9285-4ee9-cdc7-f4ce0552fda3"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='What is atomic clock?'\n",
            "content='An atomic clock is a highly accurate timekeeping device that uses the vibrations of atoms to regulate the time. Specifically, cesium atomic clocks are commonly used as they are very accurate and provide a standard for measuring time intervals. These clocks are used in national standards laboratories to maintain precise time measurements.'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing code"
      ],
      "metadata": {
        "id": "d0O9v0uNlh-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the pdf document into LangChain\n",
        "data = load_document('files/The Mahabharata of Krishna-Dwaipayana Vyasa (Complete 18 Volumes) - Kisari Mohan Ganguli.pdf')\n",
        "\n",
        "# Splitting the document into chunks\n",
        "chunks = chunk_data(data, chunk_size=256)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQtGmvx5lgK5",
        "outputId": "df5072d2-67d0-4982-898b-f92ea52d5a9c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading files/The Mahabharata of Krishna-Dwaipayana Vyasa (Complete 18 Volumes) - Kisari Mohan Ganguli.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_embedding_cost(chunks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fnwVCm8mbjs",
        "outputId": "e7b2a21a-f3cf-48f1-b366-88a182fcbc53"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Tokens: 4079197\n",
            "Embedding Cost in USD: 0.081584\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a Chroma vector store using the provided text chunks and embedding model (default is text-embedding-3-small)\n",
        "vector_store = create_embeddings_chroma(chunks)"
      ],
      "metadata": {
        "id": "wicd-Y-hl3bS"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loop for Asking Questions?"
      ],
      "metadata": {
        "id": "NHJ2KJaygjrs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    q = input('Your question: ')\n",
        "    if q.lower() in 'exit quit bye':\n",
        "        print('Bye bye!')\n",
        "        break\n",
        "    result = ask_question(q, crc)\n",
        "    print(result['answer'])\n",
        "    print('-' * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lWnaUb-gkN6N",
        "outputId": "74577388-9fc9-44a2-f88e-2c10420041df"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your question: Who is samvaran?\n",
            "Samvarana is a king mentioned in Hindu mythology. He was the husband of Tapatī, the daughter of Surya, and the father of Kuru.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Your question: Create the genology of Krishna\n",
            "Krishna is mentioned as being born of the Sattwata race and is referred to as the root of the Pandavas. He is also described as being related to Rishis Narada and Parvata, with Narada being his maternal uncle and Parvata being his sister's son. Krishna is said to be the substance of the twenty-four objects of knowledge and is named Krishna because he unites what is implied by the two words Krishi.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Your question: How is Sanjaya introduced in mahabharat\n",
            "Sanjaya is introduced in the Mahabharata as a charioteer.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Your question: References of Geeta\n",
            "I don't have information on references to Geeta in the provided context.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Your question:  how many  geeta are there in mahabharat?\n",
            "There is one Geeta in the Mahabharata.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Your question: Who writtent this book?\n",
            "I don't have enough information to determine who wrote this book.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Your question: Who written Mahabharta\n",
            "The Mahabharata is traditionally attributed to the sage Vyasa.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Your question: bye\n",
            "Bye bye!\n"
          ]
        }
      ]
    }
  ]
}