{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNCqEzSKQkDT9teEzgNfRC9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abmishra1234/LLM-Apps/blob/Development/MyCustomChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installation Required"
      ],
      "metadata": {
        "id": "RfAHzMHMeAMX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " ! pip install python-dotenv\n",
        " ! pip install langchain\n",
        " ! pip install langchain_openai"
      ],
      "metadata": {
        "id": "j1lQsbV2cAFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Required"
      ],
      "metadata": {
        "id": "EwPrMpfxeQ_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=True)\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    input_variables=[\"content\"],\n",
        "    messages=[\n",
        "#         SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
        "        SystemMessage(content='You respond only in Hindi.'),\n",
        "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "\n",
        "while True:\n",
        "    content = input('Your prompt: ')\n",
        "    if content.lower() in ['quit', 'exit', 'bye']:\n",
        "        print('Goodbye!')\n",
        "        break\n",
        "\n",
        "    response = chain.invoke({'content': content})\n",
        "    print(response)\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "id": "gdVLo_hkdO7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Chat Memory Using ConversationBufferMemory"
      ],
      "metadata": {
        "id": "TAJM9TrWhPna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=False)\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# 1. Imports\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
        "\n",
        "# 2. Create memory\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key='chat_history',\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "# 3. add  MessagesPlaceholder(variable_name='messages') to the prompt\n",
        "prompt = ChatPromptTemplate(\n",
        "    input_variables=[\"content\", \"chat_history\"],\n",
        "    messages=[\n",
        "        SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"), # Where the memory will be stored.\n",
        "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 4. Add the memory to the chain\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=memory,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "while True:\n",
        "    content = input('Your prompt: ')\n",
        "    if content.lower() in ['quit', 'exit', 'bye']:\n",
        "        print('Goodbye!')\n",
        "        break\n",
        "\n",
        "    response = chain.invoke({'content': content})\n",
        "    print(response)\n",
        "    print('-' * 50)\n"
      ],
      "metadata": {
        "id": "XE8KXZ-jhSCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the Chat Sessions"
      ],
      "metadata": {
        "id": "MAzLh4nAkDju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv, find_dotenv\n",
        "load_dotenv(find_dotenv(), override=False)\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.schema import SystemMessage\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# 1. Import FileChatMessageHistory\n",
        "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder\n",
        "\n",
        "# Why temperature is high here because I need creative AI Assistant for chat\n",
        "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
        "\n",
        "# 2. Add an additional keyword argument to the ConversationBufferMemory() constructor\n",
        "history = FileChatMessageHistory('chat_history.json') # created an empty chat_history json file\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key='chat_history',\n",
        "    chat_memory=history,\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "prompt = ChatPromptTemplate(\n",
        "    input_variables=[\"content\", \"chat_history\"],\n",
        "    messages=[\n",
        "        SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
        "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
        "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    memory=memory,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "while True:\n",
        "    content = input('Your prompt: ')\n",
        "    if content.lower() in ['quit', 'exit', 'bye']:\n",
        "        print('Goodbye!')\n",
        "        break\n",
        "\n",
        "    response = chain.invoke({'content': content})\n",
        "    print(response['text'])\n",
        "    print('-' * 50)\n"
      ],
      "metadata": {
        "id": "plxZpn0ukG5O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}